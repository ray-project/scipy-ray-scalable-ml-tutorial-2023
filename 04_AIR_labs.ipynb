{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf85d4da-7bcc-404d-8bf5-17a9366ce049",
   "metadata": {},
   "source": [
    "# Ray AIR Lab Exercises\n",
    "\n",
    "## AIR experiments: easy, medium, hard\n",
    "\n",
    "In this notebook, we will start with a minimal Ray AIR workflow and try to modify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f13ce-afd0-44fc-8127-fd310e07c3fb",
   "metadata": {},
   "source": [
    "## Starting template\n",
    "\n",
    "As a reminder and starting point, we can review the AIR workflow diagram and the minimal workflow code:\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77553c32-bd2e-40eb-8f18-11dab17ca062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import serve\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712b59d-ac72-4e19-90ae-3770636db92e",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a8d9d-f3d9-4828-99a2-aae8992092ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anonymous@anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\").repartition(16)\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210d00a-c89e-4665-9464-8aa854fffcec",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57586-2c14-45c3-b115-3f0e59919372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=4, use_gpu=False),\n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63748778-cdfa-48d3-b7b4-ffc91df7ad82",
   "metadata": {},
   "source": [
    "__Optimize hyperparams with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b69ba3-074b-40c5-8d2f-748e1f05bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            tune_config=TuneConfig(num_samples=4, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d77d8-fef2-4333-9cae-b1f5a397a87f",
   "metadata": {},
   "source": [
    "## Labs: 3 options\n",
    "\n",
    "There are three options to choose from depending on your experience and interest -- and of course if you have extra time you could try all three.\n",
    "\n",
    "### Basic lab: modify XGBoost train/tune scaling\n",
    "\n",
    "Add scale two ways\n",
    "1. Change the TuneConfig to generate more samples\n",
    "1. Observe the Ray cluster autoscale\n",
    "1. Confirm that XGBoost training is happening using multiple cores __and__ that multiple XGBoost models (trials) are being trained at the same time.\n",
    "1. Increase the ScalingConfig in the Trainer so that `num_workers` * `num_samples` is more than your total availabe CPUs and observe what happens when we try to train.\n",
    "\n",
    "### Intermediate lab: use LightGBM \n",
    "\n",
    "We can use an alternative gradient-boosting library -- Microsoft's LightGBM (https://lightgbm.readthedocs.io/en/stable/) along with Ray, instead of XGBoost.\n",
    "\n",
    "1. Look at the Ray Train documentation page -- https://docs.ray.io/en/latest/train/train.html -- and look at the differences in the XGBoost and LightGBM examples on that page\n",
    "1. You don't need to install anything: LightGBM libraries are already installed in the cluster (the `lightgbm_ray` library has the key integrations to Ray)\n",
    "1. Add the ncessary import statements\n",
    "1. Modify the code to use the necessary `Trainer` class\n",
    "1. Observe the parallel training\n",
    "    1. If you have extra time, try expanding the scale of the training as in the basic lab above\n",
    "\n",
    "### Advanced lab: use PyTorch\n",
    "\n",
    "Integrating PyTorch is a litte more involved than just using `PyTorchTrainer`\n",
    "\n",
    "First, look at the PyTorch example on the Ray Train doc page. Use that as a starting point for your code.\n",
    "\n",
    "All necessary libraries and drivers are already installed in your cluster. We'll also start with a simple multi-layer perceptron model and some PyTorch hints supplied here:\n",
    "\n",
    "For the current dataset, start with the following model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76198569-a72c-4f33-abd9-4aa3160ca187",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "layer_size = 10\n",
    "output_size = 1\n",
    "\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbcfef-8b3f-4e50-8977-161a6eae6d33",
   "metadata": {},
   "source": [
    "When preparing your training code...\n",
    "* for the loss function, use `nn.BCEWithLogitsLoss()` (since we're comparing neural net output logits to \"is_big_tip\" values that will appear as 0 or 1\n",
    "* train for one epoch -- we want to make sure the training is working and one epoch is enough to see that\n",
    "\n",
    "When preparing the tabular data, we'll need to make some adjustments to prepare the matrices that the neural net expects...\n",
    "* when the training dataset \"shard\" is loaded in the per-worker training loop function (from `session.get_dataset_shard`)\n",
    "    * it will appear as as `dict`\n",
    "    * with a key for each column in the original dataset\n",
    "    * and a corresponding value as a Torch tensor (vector) of values for that column\n",
    "* you'll need to stack/concat the vectors corresponding to the predictor columns into a matrix (using regular PyTorch APIs)\n",
    "    * and remove any other nuisance columns which might appear (take a look at a record to see what else might be present)\n",
    "\n",
    "Ray Datasets have a schema ... here is one way to work with the schema to \n",
    "You can automate the schema cleaning with code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71978554-8409-48bd-8d4c-12d8e1f0d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb0815-43c1-46a8-b8be-30f697501bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [s for s in train_dataset.schema().names if not s.startswith('_')]\n",
    "predictors.remove(\"is_big_tip\")\n",
    "predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
