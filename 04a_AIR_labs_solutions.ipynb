{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a130916b-8258-4a8d-a9ed-603a089c77e7",
   "metadata": {},
   "source": [
    "# Ray AIR Lab Solutions\n",
    "\n",
    "## Basic lab: change XGBoost scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a641b-0778-4dc3-b078-cf8396e1a0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import serve\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e57586-2c14-45c3-b115-3f0e59919372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    # num_workers controls parallelism within the training of each model\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=False), \n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b69ba3-074b-40c5-8d2f-748e1f05bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            # num_samples controls how many models are being fit simultaneously across the cluster\n",
    "            tune_config=TuneConfig(num_samples=8, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ce6cd-7a51-4ba7-ae44-0ae9ca9aa144",
   "metadata": {},
   "source": [
    "## Intermediate lab: use LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fd9e4-84c7-4378-bedc-738d45fa54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightgbm import LightGBMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5978765-0334-4455-906d-8fddeafde411",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LightGBMTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=False),\n",
    "    # the params for LightGBM are similar to those for XGBoost but not identical\n",
    "    params={ \"objective\": \"binary\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0825d-3db6-4cfc-88cf-4e3ed0bcc421",
   "metadata": {},
   "source": [
    "## Advanced lab: use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0dbd0-2277-482c-9598-b39de4259f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air import session, Checkpoint\n",
    "from ray import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284bed9-2397-4e88-9dcd-0de577feacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 6\n",
    "layer_size = 10\n",
    "output_size = 1\n",
    "num_epochs = 1\n",
    "use_gpu = False\n",
    "\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644bcc5-1f4f-4126-a6dc-6110022385ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker():\n",
    "    dataset_shard = session.get_dataset_shard(\"train\")\n",
    "    model = BasicMLP()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    # create list of predictor dimensions\n",
    "    predictors = [s for s in train_dataset.schema().names if not s.startswith('_')]\n",
    "    predictors.remove(\"is_big_tip\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batches in dataset_shard.iter_torch_batches(\n",
    "            batch_size=32, dtypes=torch.float\n",
    "        ):\n",
    "            # prepare label matrix\n",
    "            labels = batches[\"is_big_tip\"].view(-1,1)\n",
    "\n",
    "            # combine predictor columns into matrix\n",
    "            inputs = torch.vstack([batches[col] for col in predictors]).t()\n",
    "            \n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
    "\n",
    "        session.report(\n",
    "            {},\n",
    "            checkpoint=Checkpoint.from_dict(\n",
    "                dict(epoch=epoch, model=model.state_dict())\n",
    "            ),\n",
    "        )\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "result = trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
